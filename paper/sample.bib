%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Wilson Cai at 2016-12-04 14:09:39 -0800


%% Saved with string encoding Unicode (UTF-8)



@article{duchi2012ergodic,
	Author = {Duchi, John C and Agarwal, Alekh and Johansson, Mikael and Jordan, Michael I},
	Journal = {SIAM Journal on Optimization},
	Number = {4},
	Pages = {1549--1578},
	Publisher = {SIAM},
	Title = {Ergodic mirror descent},
	Volume = {22},
	Year = {2012}}

@book{aldous2002reversible,
	Author = {Aldous, David and Fill, Jim},
	Publisher = {Berkeley},
	Title = {Reversible Markov chains and random walks on graphs},
	Year = {2002}}

@book{levin2009markov,
	Author = {Levin, David Asher and Peres, Yuval and Wilmer, Elizabeth Lee},
	Publisher = {American Mathematical Soc.},
	Title = {Markov chains and mixing times},
	Year = {2009}}

@book{kushner2012stochastic,
	Author = {Kushner, Harold Joseph and Clark, Dean S},
	Publisher = {Springer Science \& Business Media},
	Title = {Stochastic approximation methods for constrained and unconstrained systems},
	Volume = {26},
	Year = {2012}}

@article{bach2014adaptivity,
	Author = {Bach, Francis R},
	Journal = {Journal of Machine Learning Research},
	Number = {1},
	Pages = {595--627},
	Title = {Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression.},
	Volume = {15},
	Year = {2014}}

@article{Tanner_The_1987,
	Abstract = {Abstract The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it },
	Author = {Tanner, Martin A and Wong, Wing},
	Doi = {10.1080/01621459.1987.10478458},
	Number = {398},
	Pages = {528-540},
	Title = {The calculation of posterior distributions by data augmentation},
	Volume = {82},
	Year = {1987},
}
@article{Rubin_The_1981,
	Abstract = {Abstract The Bayesian bootstrap is the Bayesian analogue of the bootstrap. Instead of simulating the sampling distribution of a statistic estimating a parameter, the Bayesian bootstrap simulates the posterior distribution of the parameter; operationally and inferentially },
	Author = {Rubin, {DB}},
	Doi = {10.1307/aos/1176345338},
	Journal = {The annals of statistics},
	Title = {The bayesian bootstrap},
	Year = {1981},
}

@article{Geman_Stochastic_1993,
	Abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field {(MRF)} equivalence, this assignment also determines an {MRF} image model. The energy  ...},
	Author = {Geman, S and Geman, D},
	Doi = {10.1080/02664769300000058},
	Journal = {Journal of Applied Statistics},
	Title = {Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images*},
	Year = {1993},
}

@article{Dempster_Maximum_1977,
	Abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, },
	Author = {Dempster, {AP} and Laird, {NM} and Rubin, {DB}},
	Journal = {Journal of the royal statistical society. {\ldots}},
	Title = {Maximum likelihood from incomplete data via the {EM} algorithm},
	Year = {1977}}

@article{Wei_A_1990,
	Abstract = {Abstract The first part of this article presents the Monte Carlo implementation of the E step of the {EM} algorithm. Given the current guess to the maximizer of the posterior distribution, latent data patterns are generated from the conditional predictive distribution. The expected },
	Author = {Wei, {GCG} and Tanner, {MA}},
	Doi = {10.1080/01621459.1990.10474930},
	Journal = {Journal of the American statistical {\ldots}},
	Title = {A Monte Carlo implementation of the {EM} algorithm and the poor man's data augmentation algorithms},
	Year = {1990},
    }
@article{Robbins_A_1951,
  pages={400-407},
  title={A stochastic approximation method},
  doi={10.1307/aoms/1177729586},
  author={Robbins, Herbert and Monro, Sutton},
  year={1951},
  abstract={Let M (x) denote the expected value at level x of the response to a certain experiment. M (x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x= θ of the equation M (x)= α, where α is a given constant. We give a method for making successive experiments at levels x1, x2,⋯ in such a way that xn will tend to θ in probability.}
}
@article{Polyak_Acceleration_1992,
  title={Acceleration of stochastic approximation by averaging},
  journal={{SIAM} Journal on Control and Optimization},
  author={Polyak, {BT} and Juditsky, {AB}},
  year={1992},
  abstract={A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.}
}
@article{Nedic_Incremental_2001,
  title={Incremental subgradient methods for nondifferentiable optimization},
  journal={{SIAM} Journal on Optimization},
  author={Nedic, A and Bertsekas, {DP}},
  year={2001},
  abstract={We consider a class of subgradient methods for minimizing a convex function that consists of the sum of a large number of component functions. This type of minimization arises in a dual context from Lagrangian relaxation of the coupling constraints of large scale separable problems. The idea is to perform the subgradient iteration incrementally, by sequentially taking steps along the subgradients of the component functions, with intermediate  ...}
}
@article{LeCun_Efficient_2012,
  title={Efficient backprop},
  journal={Neural networks: Tricks of the  …},
  author={{LeCun,} {YA} and Bottou, L and Orr, {GB} and M{\"u}ller, {KR}},
  year={2012},
  abstract={... Volume 7700 of the series Lecture Notes in Computer Science pp 9-48. Efficient {BackProp.} ... Ideally
we want to remove one of the inputs which will decrease the size of the network. ... Nonlinear
activation functions are what give neural networks their nonlinear capabilities. ...
}
}
@article{Finkel_Efficient_2008,
  title={Efficient, Feature-based, Conditional Random Field Parsing.},
  journal={{ACL}},
  author={Finkel, {JR} and Kleeman, A and Manning, {CD}},
  year={2008},
  abstract={Abstract Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full  ...}
}
